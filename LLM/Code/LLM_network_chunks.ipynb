{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pdfplumber\n",
    "import docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE DIRECTORIES\n",
    "\n",
    "# Define the project root directory\n",
    "root_dir = '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/'\n",
    "\n",
    "# File paths\n",
    "report1_dir = '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data/REPORT_1/report1_trimmed.pdf'\n",
    "report2_dir = '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data/REPORT_2/access_20250310'\n",
    "target_data_dir  = '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data'\n",
    "\n",
    "# Define absolute python path\n",
    "sys.path.insert(0, root_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT FUNCTIONS & DATA\n",
    "\n",
    "\n",
    "# Load API and import functions\n",
    "from Code.API import get_chat_response, num_tokens_from_string\n",
    "\n",
    "\n",
    "# Import all target data (target_code + target_content)\n",
    "target_data_250 = pd.read_csv(f'{target_data_dir}/targets_data_250.csv', sep=\";\")  #extensive target list from target_NACE_classification.xlsx\n",
    "target_data_150 = pd.read_csv(f'{target_data_dir}/targets_data_150.csv', sep=\";\")  #target list as in report 1\n",
    "\n",
    "\n",
    "# Import & parse report1\n",
    "with pdfplumber.open(report1_dir) as pdf:\n",
    "    # Extract the text from the PDF\n",
    "    report1 = \"\"\n",
    "    for page in pdf.pages:\n",
    "        report1 += page.extract_text()\n",
    "\n",
    "# Clean-up report1\n",
    "report1 = report1.strip() \n",
    "report1 = report1.replace(\"\\n\", \" \")\n",
    "report1 = report1.replace(\"\\t\", \" \")\n",
    "\n",
    "# Import & parse report2\n",
    "\n",
    "# create a dictionary to access different chapters of report2   \n",
    "report2 = { \n",
    "    'chapter1': docx2txt.process(f'{report2_dir}/NEW_Chapter1_CLEAN - Introduction & setting the scene_LM_trimmed.docx'),\n",
    "    'chapter2': docx2txt.process(f'{report2_dir}/NEW_Chapter2 (ex chp3) - Environmental impacts_ZOTERO_trimmed.docx'),\n",
    "    'chapter3': docx2txt.process(f'{report2_dir}/NEW_Chapter3 (ex chp4) with BIBLIO - Challenges and enablers for EGD objectives_trimmed.docx'),\n",
    "    'chapter4': docx2txt.process(f'{report2_dir}/NEW_Chapter4 (ex chp5) - Enabling the green transition_trimmed.docx'),\n",
    "    'chapter5': docx2txt.process(f'{report2_dir}/NEW_Chapter5 - Fair and just transition_trimmed.docx'),\n",
    "    'chapter6': docx2txt.process(f'{report2_dir}/NEW_Chapter6 - Financing the green transition.docx')\n",
    "}\n",
    "\n",
    "# clean up report2 chapters\n",
    "for chapter, text in report2.items():\n",
    "    report2[chapter] = text.strip()\n",
    "    report2[chapter] = report2[chapter].replace(\"\\n\", \" \")\n",
    "    report2[chapter] = report2[chapter].replace(\"\\t\", \" \")\n",
    "\n",
    "\n",
    "# Import a list of subthemes that were manually selected based on my Obsidian Canvases and Report 1.\n",
    "from Data.subthemes import subthemes_list\n",
    "subthemes_list = subthemes_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHUNK GENERATION\n",
    "\n",
    "# Parameters for chunk generation\n",
    "chunks = 10  # 10 chunks give prompts of aproximately 85k tokens (based on trial&error)\n",
    "data_to_split = subthemes_list\n",
    "\n",
    "# Generate chunks \n",
    "target_data_chunks = np.array_split(ary=data_to_split, indices_or_sections=chunks)\n",
    "\n",
    "# Generate all potential pairs of chunks\n",
    "chunk_pairs = {}\n",
    "chunk_list = list(range(chunks))\n",
    "chunk_id = 0\n",
    "\n",
    "for i in range(len(chunk_list)):\n",
    "    for j in range(len(chunk_list)):\n",
    "        if i != j:\n",
    "            chunk_pairs[chunk_id] = [chunk_list[i], chunk_list[j]]\n",
    "            chunk_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE ANSWERS\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "seed = None \n",
    "temperature = 0.1\n",
    "model = \"llama-3.3-70b-instruct\" #\"llama-3.3-70b-instruct\" \"gpt-4o\" \"nous-hermes-2-mixtral-8x7b-dpo\"\n",
    "date = '0319' \n",
    "output_directory = f'/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data/Outputs/{date}/'\n",
    "os.makedirs(output_directory, exist_ok=True) # Create the 'date' folder if it doesn't exist\n",
    "\n",
    "\n",
    "\n",
    "# (loop tools and formatting)\n",
    "answers_metadata = pd.DataFrame(columns=[\"chunk_pairs_nbr\",\n",
    "                                         \"chunk_pairs\",        # create empty panda dataframe with the following columns so to gather a bit more data on the responses and ultimately try to assess consistency\n",
    "                                         \"model\",\n",
    "                                         \"seed\",\n",
    "                                         \"temperature\",\n",
    "                                         \"system_fingerprint\", \n",
    "                                         \"prompt_tokens\", \n",
    "                                         \"completion_tokens\"])  \n",
    "\n",
    "\n",
    "# Loop\n",
    "for x in range(len(chunk_pairs)): \n",
    "#for x in range(34, 90):\n",
    "#for x in list([56,57]):\n",
    "\n",
    "    success = False  # Initialize a flag to track whether the operation was successful\n",
    "    retry_count = 0  # Initialize a counter to track the number of retries\n",
    "    max_retries = 5  # adjust this value to set the desired number of retries\n",
    "\n",
    "    while not success and retry_count < max_retries:\n",
    "        try:\n",
    "            # Subset data to avoid overloading the model\n",
    "            sub1 = [f\"{row['target_code']}: {row['target_content']}. Assessment: {row['target_assessment']}\" for index, row in target_data_chunks[chunk_pairs[x][0]].iterrows()] # select chunk of targets  corresponding to the 1st element ('0') of the chunk pair number 'x' and concatenate target_code and respective target_content into a list of strings, so it can be added to the prompts\n",
    "            sub2 = [f\"{row['target_code']}: {row['target_content']}. Assessment: {row['target_assessment']}\" for index, row in target_data_chunks[chunk_pairs[x][1]].iterrows()] # select chunk of targets corresponding to the 2d element ('1') of the chunk pair number 'x', and concatenate target_code and respective target_content into a list of strings, so it can be added to the prompts\n",
    "\n",
    "\n",
    "            # Define prompt\n",
    "            prompt = f'''\n",
    "                Data input & Context:\n",
    "                - List of European Green Deal (EGD) targets A: {sub1}\n",
    "                - List of European Green Deal (EGD) targets B: {sub2}\n",
    "                - Report n°2: {report2['chapter1']} + {report2['chapter2']} + {report2['chapter3']} + {report2['chapter4']} + {report2['chapter5']}\n",
    "\n",
    "                Task: \n",
    "                - Based on the content of Report n°2 and the content of EGD targets in list A and list B (i.e., their content and assessment), determine which targets in list A are likely to have a positive or negative impact on targets in list B, and reversely.\n",
    "                - Determine which sub-themes in list A are likely to have a positive or negative impact on sub-themes in list B, and reversely.\n",
    "\n",
    "                Answer format: provide your answer as a table in csv format please (separator: \";\"), with the following columns:\n",
    "                - source_subtheme (e.g., GHG Reduction)\n",
    "                - source_subtheme_targets (e.g.,TA1.3,TA1.7,TA1.9,TA1.11,TA1.13,TA5.7) \n",
    "                - impact_subtheme (the name of the subtheme that is likely to be positively or negatively affected by the implementation and requirements of the subtheme in the 'source_subtheme' column)\n",
    "                - impact_type (positive '+' or negative '-')\n",
    "                - justification\n",
    "\n",
    "                Specifications:\n",
    "                - If some sub-themes do not have any connections at all (i.e., are isolated), still add them but add 'NA' to the impact_subtheme and impact_type columns.\n",
    "                - One row per connection, if you deem that one sub-theme has an impact on multiple other sub-themes, add as many rows for a same subtheme as necessary.\n",
    "                - Ensure that your analysis accounts for both directionalities.\n",
    "                - It is critical that your analysis is based on the context of the report and not just on the semantics of the target contents.\n",
    "                - Don't forget negative connections as well.\n",
    "                - This is mandatory: for each sub-theme connection, write one to two sentences justifying your choice. \n",
    "                - Output only the CSV table. Do not include additional commentary.\n",
    "            '''\n",
    "\n",
    "            # Print pre-generation metadata (to double check amount of tokens in prompt, JRC llama3.3 should have a max of 120k)\n",
    "            prompt_metadata = f'''Chunks pair: {x} \\nChunks: {chunk_pairs[x]} \\nPrompt length: {len(prompt)} \\nPrompt tokens (o200k_base encoding): {num_tokens_from_string(prompt, \"o200k_base\")} \\nPrompt tokens (cl100k_base encoding): {num_tokens_from_string(prompt, \"cl100k_base\")} \\n'''\n",
    "            print(prompt_metadata)\n",
    "\n",
    "            # Generate answer\n",
    "            answer = get_chat_response(prompt=prompt,\n",
    "                                      seed=seed,\n",
    "                                      model=model,\n",
    "                                      temperature=temperature)\n",
    "\n",
    "            # Print post-generation metadata \n",
    "            print(f'Answer generated.')\n",
    "            print(f'Prompt tokens: {answer[\"prompt_tokens\"]} \\nCompletion tokens: {answer[\"completion_tokens\"]}')\n",
    "\n",
    "\n",
    "            # Add the metadata of the generated answer to a dataframe\n",
    "            answers_metadata.loc[x] = (x,\n",
    "                                       chunk_pairs[x],\n",
    "                                       model,\n",
    "                                       seed,\n",
    "                                       temperature,\n",
    "                                       answer[\"system_fingerprint\"],\n",
    "                                       answer[\"prompt_tokens\"],\n",
    "                                       answer[\"completion_tokens\"])\n",
    "\n",
    "            # Save the generated answer as a CSV file\n",
    "            output_name = f'{date}_network_pair{x}.csv'\n",
    "\n",
    "            with open((os.path.join(output_directory, output_name)), 'w') as f:\n",
    "                f.write(answer[\"response_content\"])\n",
    "\n",
    "            # If success, set the success flag to True\n",
    "            success = True\n",
    "\n",
    "            # If success, add a 2-minute pause between answer requests to avoid RateLimitErrors\n",
    "            print(f\"-- 1 min pause \\n\")\n",
    "            time.sleep(60)\n",
    "\n",
    "        except Exception as e:\n",
    "            \n",
    "            retry_count += 1  # Increment the retry counter if an error occurs\n",
    "            error_type = type(e).__name__  # Get the type of error that occurred\n",
    "            error_message = str(e) # Get the error message\n",
    "            print(f\"An error occurred ({error_type}): {error_message}. Retrying ({retry_count}/{max_retries})\")  # Print an error message with the type and message\n",
    "\n",
    "\n",
    "    # Print a message if the operation failed after the maximum number of retries\n",
    "    if not success:\n",
    "        print(f\"Failed to generate answer for pair{x} ({chunk_pairs[x]}) after {max_retries} retries.\")\n",
    "\n",
    "# Save the metadata dataframe as a CSV file\n",
    "answers_metadata.to_csv(path_or_buf=os.path.join(output_directory, f'{date}_network_metadata.csv'),\n",
    "                         sep=';',\n",
    "                         index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
