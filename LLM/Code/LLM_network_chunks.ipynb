{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pdfplumber\n",
    "import docx2txt\n",
    "\n",
    "\n",
    "# Define absolute python path\n",
    "sys.path.insert(0, '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/') \n",
    "\n",
    "\n",
    "## FUNCTIONS\n",
    "\n",
    "# Load API and import request function\n",
    "from Code.API import get_chat_response, num_tokens_from_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "\n",
    "target_data_directory = '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data'\n",
    "report1_directory = '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data/REPORT_1/report1_trimmed.pdf'\n",
    "report2_directory = '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data/REPORT_2/access_20250310'\n",
    "\n",
    "# Import all target data (target_code + target_content)\n",
    "target_data_254 = pd.read_csv(f'{target_data_directory}/targets_data_254.csv', sep=\";\")  #extensive target list from target_NACE_classification.xlsx\n",
    "target_data_150 = pd.read_csv(f'{target_data_directory}/targets_data_150.csv', sep=\";\")  #target list as in report 1\n",
    "\n",
    "\n",
    "# Import & parse report1\n",
    "with pdfplumber.open(report1_directory) as pdf:\n",
    "    # Extract the text from the PDF\n",
    "    report1 = \"\"\n",
    "    for page in pdf.pages:\n",
    "        report1 += page.extract_text()\n",
    "\n",
    "# Clean-up report1\n",
    "report1 = report1.strip() \n",
    "report1 = report1.replace(\"\\n\", \" \")\n",
    "report1 = report1.replace(\"\\t\", \" \")\n",
    "\n",
    "# Import & parse report2\n",
    "\n",
    "# create a dictionary to access different chapters of report2   \n",
    "report2 = { \n",
    "    'chapter1': docx2txt.process(f'{report2_directory}/NEW_Chapter1_CLEAN - Introduction & setting the scene_LM_trimmed.docx'),\n",
    "    'chapter2': docx2txt.process(f'{report2_directory}/NEW_Chapter2 (ex chp3) - Environmental impacts_ZOTERO_trimmed.docx'),\n",
    "    'chapter3': docx2txt.process(f'{report2_directory}/NEW_Chapter3 (ex chp4) with BIBLIO - Challenges and enablers for EGD objectives_trimmed.docx'),\n",
    "    'chapter4': docx2txt.process(f'{report2_directory}/NEW_Chapter4 (ex chp5) - Enabling the green transition_trimmed.docx'),\n",
    "    'chapter5': docx2txt.process(f'{report2_directory}/NEW_Chapter5 - Fair and just transition_trimmed.docx'),\n",
    "    'chapter6': docx2txt.process(f'{report2_directory}/NEW_Chapter6 - Financing the green transition.docx')\n",
    "}\n",
    "\n",
    "# clean up report2 chapters\n",
    "for chapter, text in report2.items():\n",
    "    report2[chapter] = text.strip()\n",
    "    report2[chapter] = report2[chapter].replace(\"\\n\", \" \")\n",
    "    report2[chapter] = report2[chapter].replace(\"\\t\", \" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subthemes\n",
    "\n",
    "# Assuming this is your dataset\n",
    "data = target_data_150\n",
    "\n",
    "\n",
    "# Create a dictionary to store the DataFrames\n",
    "subthemes_data_dict = {}\n",
    "\n",
    "# Iterate over the unique 'sub_theme' values\n",
    "for theme in data['sub_theme'].unique():\n",
    "    # Filter the DataFrame for the current 'sub_theme'\n",
    "    theme_df = data[data['sub_theme'] == theme][['target_code', 'target_content', 'target_assessment']]\n",
    "    \n",
    "    # Add the filtered DataFrame to the dictionary\n",
    "    subthemes_data_dict[theme] = theme_df\n",
    "\n",
    "\n",
    "#print(f\"{subthemes_data_dict['Climate Resilience']}\")\n",
    "\n",
    "\n",
    "\n",
    "sub_themes = [\n",
    "\t\n",
    "\t\"Climate Resilience\",\n",
    "\t\"GHG Reduction\",\n",
    "\t\"GHG Reduction - Buildings\",\n",
    "\t\"GHG Reduction - Transports\"\n",
    "\t\"GHG Removal\",\n",
    "\t\"Renewable Energy\",\n",
    "\t\"Renewable Energy - Heating & Cooling\",\n",
    "\t\"Renewable Energy - Hydrogen Production\",\n",
    "\t\"Renewable Energy - Ocean/Offshore\",\n",
    "\t\"Renewable Energy - Solar\",\n",
    "\t\"Energy Efficiency\",\n",
    "\t\"Energy Efficiency - Buildings\",\n",
    "\t\"Energy Infrastructure\",\n",
    "\t\"Methane\",\n",
    "\t\"Social Security - Energy\",\n",
    "\t\"Waste Reduction\",\n",
    "\t\"Waste Reduction - Municipal Waste\",\n",
    "\t\"Waste Reduction - Food Waste\",\n",
    "\t\"Waste Reduction - Plastic & Packaging\",\n",
    "\t\"Circularity/Recycling\",\n",
    "\t\"Circularity/Recycling - Municipal Waste\",\n",
    "\t\"Circularity/Recycling - Textile Waste\",\n",
    "\t\"Circularity/Recycling - Plastic & Packaging\",\n",
    "\t\"Circularity/Recycling - Plastic & Packaging - Bio-based plastics\",\n",
    "\t\"Circularity/Recycling - Vehicle Circularity\",\n",
    "\t\"Circularity/Recycling - Critical Raw Materials - Batteries Recycling\",\n",
    "\t\"Critical Raw Materials - Extraction & Import\",\n",
    "\t\"Net-Zero Technology - Manufacturing\",\n",
    "\t\"Rail\",\n",
    "\t\"Net-Zero Technology - Road Vehicles\",\n",
    "\t\"Net-Zero Technology - Maritime Transport\",\n",
    "\t\"Net-Zero Technology - Aviation\",\n",
    "\t\"Biofuels\",\n",
    "\t\"Other Low-Carbon Fuels\",\n",
    "\t\"Hydrogen Distribution\",\n",
    "\t\"Urban Mobility\",\n",
    "\t\"Transport Logistics\",\n",
    "\t\"Food quality\",\n",
    "\t\"Food quality - Animal Welfare\",\n",
    "\t\"Food quality - Healthy Food\",\n",
    "\t\"Food affordability\",\n",
    "    \"Social Security - Workers Protection\",\n",
    "\t\"Pesticides Reduction\",\n",
    "\t\"Competitive Agriculture\",\n",
    "\t\"Terrestrial Ecosystems Restoration\",\n",
    "\t\"Terrestrial Ecosystems Restoration - Rivers\",\n",
    "\t\"Terrestrial Ecosystems Restoration - Agricultural Ecosystems\",\n",
    "\t\"Terrestrial Ecosystems Restoration - Forests\",\n",
    "\t\"Marine Ecosystem Restoration\",\n",
    "\t\"Biodiversity Protection & Conservation\",\n",
    "\t\"Biodiversity Protection & Conservation - Fisheries\",\n",
    "\t\"Biodiversity Protection & Conservation - Monitoring\",\n",
    "\t\"Biodiversity Protection & Conservation - Urban Nature\",\n",
    "\t\"Forest Bioeconomy\",\n",
    "\t\"Improve Air Quality\",\n",
    "\t\"Improve Water Quality\",\n",
    "\t\"Improve Soils Health\",\n",
    "\t\"Noise Reduction\",\n",
    "    \"Social Security - Sanitation\"\n",
    "\t\n",
    "\t]\n",
    "\n",
    "sub_themes_per_TA = {\n",
    "    \"TA1\": [\n",
    "        \"Climate Resilience\",\n",
    "        \"GHG Reduction\",\n",
    "        \"GHG Reduction - Buildings\",\n",
    "        \"GHG Reduction - Transports\",\n",
    "        \"GHG Removal\",  \n",
    "    ],\n",
    "    \"TA2\": [\n",
    "        \"Renewable Energy\",\n",
    "        \"Renewable Energy - Heating & Cooling\",\n",
    "        \"Renewable Energy - Hydrogen Production\",\n",
    "        \"Renewable Energy - Ocean/Offshore\",\n",
    "        \"Renewable Energy - Solar\",\n",
    "        \"Energy Efficiency\",\n",
    "        \"Energy Efficiency - Buildings\",\n",
    "        \"Energy Infrastructure\",\n",
    "        \"Methane\",\n",
    "        \"Social Security - Energy\",   \n",
    "    ],\n",
    "    \"TA3\": [\n",
    "        \"Waste Reduction\",\n",
    "        \"Waste Reduction - Municipal Waste\",\n",
    "        \"Waste Reduction - Food Waste\",\n",
    "        \"Waste Reduction - Plastic & Packaging\",\n",
    "        \"Circularity/Recycling\",\n",
    "        \"Circularity/Recycling - Municipal Waste\",\n",
    "        \"Circularity/Recycling - Textile Waste\",\n",
    "        \"Circularity/Recycling - Plastic & Packaging\",\n",
    "        \"Circularity/Recycling - Plastic & Packaging - Bio-based plastics\",\n",
    "        \"Circularity/Recycling - Vehicle Circularity\",\n",
    "        \"Circularity/Recycling - Critical Raw Materials - Batteries Recycling\",\n",
    "        \"Critical Raw Materials - Extraction & Import\",\n",
    "        \"Net-Zero Technology - Manufacturing\",\n",
    "    ],\n",
    "    \"TA4\": [\n",
    "        \"Rail\",\n",
    "        \"Net-Zero Technology - Road Vehicles\",\n",
    "        \"Other Low-Carbon Fuels\",\n",
    "        \"Biofuels\",\n",
    "        \"Hydrogen Distribution\",\n",
    "        \"Urban Mobility\",\n",
    "        \"Transport Logistics\",\n",
    "    ],\n",
    "    \"TA5\": [\n",
    "        \"Food quality\",\n",
    "        \"Food quality - Animal Welfare\",\n",
    "        \"Food quality - Healthy Food\",\n",
    "        \"Food affordability\",\n",
    "        \"Pesticides Reduction\",\n",
    "        \"Competitive Agriculture\",\n",
    "    ],\n",
    "    \"TA6\": [\n",
    "        \"Terrestrial Ecosystems Restoration\",\n",
    "        \"Terrestrial Ecosystems Restoration - Rivers\",\n",
    "        \"Terrestrial Ecosystems Restoration - Agricultural Ecosystems\",\n",
    "        \"Terrestrial Ecosystems Restoration - Forests\",\n",
    "        \"Marine Ecosystem Restoration\",\n",
    "        \"Biodiversity Protection & Conservation\",\n",
    "        \"Biodiversity Protection & Conservation - Fisheries\",\n",
    "        \"Biodiversity Protection & Conservation - Monitoring\",\n",
    "        \"Biodiversity Protection & Conservation - Urban Nature\",  \n",
    "    ],\n",
    "    \"TA7\": [\n",
    "        \"Improve Air Quality\",\n",
    "        \"Improve Water Quality\",\n",
    "        \"Improve Soils Health\",\n",
    "        \"Noise Reduction\",\n",
    "        \"Social Security - Sanitation\",   \n",
    "    ],\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for chunk generation\n",
    "chunks = 10  # 10 chunks give prompts of aproximately 85k tokens (based on trial&error)\n",
    "data_to_split = sub_themes\n",
    "\n",
    "\n",
    "\n",
    "# Generate chunks \n",
    "target_data_chunks = np.array_split(ary=data_to_split, indices_or_sections=chunks)\n",
    "\n",
    "# Generate all potential pairs of chunks\n",
    "chunk_pairs = {}\n",
    "chunk_list = list(range(chunks))\n",
    "chunk_id = 0\n",
    "\n",
    "for i in range(len(chunk_list)):\n",
    "    for j in range(len(chunk_list)):\n",
    "        if i != j:\n",
    "            chunk_pairs[chunk_id] = [chunk_list[i], chunk_list[j]]\n",
    "            chunk_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE ANSWERS\n",
    "\n",
    "\n",
    "# Set parameters\n",
    "seed = None \n",
    "temperature = 0.1\n",
    "model = \"llama-3.3-70b-instruct\" #\"llama-3.3-70b-instruct\" \"gpt-4o\" \"nous-hermes-2-mixtral-8x7b-dpo\"\n",
    "date = '0319' \n",
    "output_directory = f'/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data/Outputs/{date}/'\n",
    "os.makedirs(output_directory, exist_ok=True) # Create the 'date' folder if it doesn't exist\n",
    "\n",
    "\n",
    "\n",
    "# (loop tools and formatting)\n",
    "answers_metadata = pd.DataFrame(columns=[\"chunk_pairs_nbr\",\n",
    "                                         \"chunk_pairs\",        # create empty panda dataframe with the following columns so to gather a bit more data on the responses and ultimately try to assess consistency\n",
    "                                         \"model\",\n",
    "                                         \"seed\",\n",
    "                                         \"temperature\",\n",
    "                                         \"system_fingerprint\", \n",
    "                                         \"prompt_tokens\", \n",
    "                                         \"completion_tokens\"])  \n",
    "\n",
    "\n",
    "# Loop\n",
    "for x in range(len(chunk_pairs)): \n",
    "#for x in range(34, 90):\n",
    "#for x in list([56,57]):\n",
    "\n",
    "    success = False  # Initialize a flag to track whether the operation was successful\n",
    "    retry_count = 0  # Initialize a counter to track the number of retries\n",
    "    max_retries = 5  # adjust this value to set the desired number of retries\n",
    "\n",
    "    while not success and retry_count < max_retries:\n",
    "        try:\n",
    "            # Subset data to avoid overloading the model\n",
    "            sub1 = [f\"{row['target_code']}: {row['target_content']}. Assessment: {row['target_assessment']}\" for index, row in target_data_chunks[chunk_pairs[x][0]].iterrows()] # select chunk of targets  corresponding to the 1st element ('0') of the chunk pair number 'x' and concatenate target_code and respective target_content into a list of strings, so it can be added to the prompts\n",
    "            sub2 = [f\"{row['target_code']}: {row['target_content']}. Assessment: {row['target_assessment']}\" for index, row in target_data_chunks[chunk_pairs[x][1]].iterrows()] # select chunk of targets corresponding to the 2d element ('1') of the chunk pair number 'x', and concatenate target_code and respective target_content into a list of strings, so it can be added to the prompts\n",
    "\n",
    "\n",
    "            # Define prompt\n",
    "            prompt = f'''\n",
    "                Data input & Context:\n",
    "                - List of European Green Deal (EGD) targets A: {sub1}\n",
    "                - List of European Green Deal (EGD) targets B: {sub2}\n",
    "                - Report n°2: {report2['chapter1']} + {report2['chapter2']} + {report2['chapter3']} + {report2['chapter4']} + {report2['chapter5']}\n",
    "\n",
    "                Task: \n",
    "                - Based on the content of Report n°2 and the content of EGD targets in list A and list B (i.e., their content and assessment), determine which targets in list A are likely to have a positive or negative impact on targets in list B, and reversely.\n",
    "                - Determine which sub-themes in list A are likely to have a positive or negative impact on sub-themes in list B, and reversely.\n",
    "\n",
    "                Answer format: provide your answer as a table in csv format please (separator: \";\"), with the following columns:\n",
    "                - source_subtheme (e.g., GHG Reduction)\n",
    "                - source_subtheme_targets (e.g.,TA1.3,TA1.7,TA1.9,TA1.11,TA1.13,TA5.7) \n",
    "                - impact_subtheme (the name of the subtheme that is likely to be positively or negatively affected by the implementation and requirements of the subtheme in the 'source_subtheme' column)\n",
    "                - impact_type (positive '+' or negative '-')\n",
    "                - justification\n",
    "\n",
    "                Specifications:\n",
    "                - If some sub-themes do not have any connections at all (i.e., are isolated), still add them but add 'NA' to the impact_subtheme and impact_type columns.\n",
    "                - One row per connection, if you deem that one sub-theme has an impact on multiple other sub-themes, add as many rows for a same subtheme as necessary.\n",
    "                - Ensure that your analysis accounts for both directionalities.\n",
    "                - It is critical that your analysis is based on the context of the report and not just on the semantics of the target contents.\n",
    "                - Don't forget negative connections as well.\n",
    "                - This is mandatory: for each sub-theme connection, write one to two sentences justifying your choice. \n",
    "                - Output only the CSV table. Do not include additional commentary.\n",
    "            '''\n",
    "\n",
    "            # Print pre-generation metadata (to double check amount of tokens in prompt, JRC llama3.3 should have a max of 120k)\n",
    "            prompt_metadata = f'''Chunks pair: {x} \\nChunks: {chunk_pairs[x]} \\nPrompt length: {len(prompt)} \\nPrompt tokens (o200k_base encoding): {num_tokens_from_string(prompt, \"o200k_base\")} \\nPrompt tokens (cl100k_base encoding): {num_tokens_from_string(prompt, \"cl100k_base\")} \\n'''\n",
    "            print(prompt_metadata)\n",
    "\n",
    "            # Generate answer\n",
    "            answer = get_chat_response(prompt=prompt,\n",
    "                                      seed=seed,\n",
    "                                      model=model,\n",
    "                                      temperature=temperature)\n",
    "\n",
    "            # Print post-generation metadata \n",
    "            print(f'Answer generated.')\n",
    "            print(f'Prompt tokens: {answer[\"prompt_tokens\"]} \\nCompletion tokens: {answer[\"completion_tokens\"]}')\n",
    "\n",
    "\n",
    "            # Add the metadata of the generated answer to a dataframe\n",
    "            answers_metadata.loc[x] = (x,\n",
    "                                       chunk_pairs[x],\n",
    "                                       model,\n",
    "                                       seed,\n",
    "                                       temperature,\n",
    "                                       answer[\"system_fingerprint\"],\n",
    "                                       answer[\"prompt_tokens\"],\n",
    "                                       answer[\"completion_tokens\"])\n",
    "\n",
    "            # Save the generated answer as a CSV file\n",
    "            output_name = f'{date}_network_pair{x}.csv'\n",
    "\n",
    "            with open((os.path.join(output_directory, output_name)), 'w') as f:\n",
    "                f.write(answer[\"response_content\"])\n",
    "\n",
    "            # If success, set the success flag to True\n",
    "            success = True\n",
    "\n",
    "            # If success, add a 2-minute pause between answer requests to avoid RateLimitErrors\n",
    "            print(f\"-- 1 min pause \\n\")\n",
    "            time.sleep(60)\n",
    "\n",
    "        except Exception as e:\n",
    "            \n",
    "            retry_count += 1  # Increment the retry counter if an error occurs\n",
    "            error_type = type(e).__name__  # Get the type of error that occurred\n",
    "            error_message = str(e) # Get the error message\n",
    "            print(f\"An error occurred ({error_type}): {error_message}. Retrying ({retry_count}/{max_retries})\")  # Print an error message with the type and message\n",
    "\n",
    "\n",
    "    # Print a message if the operation failed after the maximum number of retries\n",
    "    if not success:\n",
    "        print(f\"Failed to generate answer for pair{x} ({chunk_pairs[x]}) after {max_retries} retries.\")\n",
    "\n",
    "# Save the metadata dataframe as a CSV file\n",
    "answers_metadata.to_csv(path_or_buf=os.path.join(output_directory, f'{date}_network_metadata.csv'),\n",
    "                         sep=';',\n",
    "                         index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  target_code                                     target_content  \\\n",
      "1       TA1.3  Reduce by at least 55% net GHG emissions compa...   \n",
      "\n",
      "                                   target_assessment  \n",
      "1  The at least 55% economy-wide net reduction ob...  \n"
     ]
    }
   ],
   "source": [
    "### SANDBOX\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming this is your dataset\n",
    "df = target_data_150\n",
    "\n",
    "\n",
    "# Create a dictionary to store the DataFrames\n",
    "subthemes_data_dict = {}\n",
    "\n",
    "# Iterate over the unique 'sub_theme' values\n",
    "for theme in df['sub_theme'].unique():\n",
    "    # Filter the DataFrame for the current 'sub_theme'\n",
    "    theme_df = df[df['sub_theme'] == theme][['target_code', 'target_content', 'target_assessment']]\n",
    "    \n",
    "    # Add the filtered DataFrame to the dictionary\n",
    "    subthemes_data_dict[theme] = theme_df\n",
    "\n",
    "\n",
    "print(f\"{subthemes_data_dict['Climate Resilience']}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
