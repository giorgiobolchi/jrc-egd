{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    " \n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "# Define absolute python path\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API and import functions\n",
    "from Code.API import get_chat_response, num_tokens_from_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "\n",
    "## DATA\n",
    "\n",
    "# Load NACE data\n",
    "from Data.NACEdata import NACElevel0, NACElevel1, NACElevel2, NACElevel3     \n",
    "\n",
    "\n",
    "# Import report1 and report1_annexes as pdf and covnert to plain text\n",
    "\n",
    "with pdfplumber.open('/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data/report1.pdf') as pdf:\n",
    "    # Extract the text from the PDF\n",
    "    report1 = \"\"\n",
    "    for page in pdf.pages:\n",
    "        report1 += page.extract_text()\n",
    "\n",
    "with pdfplumber.open('/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data/report1_annex.pdf') as pdf:\n",
    "    # Extract the text from the PDF\n",
    "    report1_annex = \"\"\n",
    "    for page in pdf.pages:\n",
    "        report1_annex += page.extract_text()\n",
    "\n",
    "    # Preprocess the text\n",
    "report1 = report1.strip()\n",
    "report1 = report1.replace(\"\\n\", \" \")\n",
    "report1 = report1.replace(\"\\t\", \" \")\n",
    "\n",
    "report1_annex = report1_annex.strip()\n",
    "report1_annex = report1_annex.replace(\"\\n\", \" \")\n",
    "report1_annex = report1_annex.replace(\"\\t\", \" \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all target data (target_code + target_content)\n",
    "targets_pd = pd.read_csv('/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data/XLSX_target_data_v1.2_LLM.csv', sep=\";\")\n",
    "\n",
    "# Split targets list into smaller chunks so that each can be processed by the AI (which has a limit of 120k tokens per run)\n",
    "\n",
    "chunks = 5\n",
    "targets_chunks = np.array_split(ary=targets_pd, indices_or_sections=chunks) # = splits the targets_pd dataframe into 10 slices\n",
    "\n",
    "\n",
    "print(f'Chunks: {len(targets_chunks)} \\nTargets per chunk: {len(targets_chunks[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(targets_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE ANSWERS\n",
    "\n",
    "\n",
    "# Select data to loop through\n",
    "\n",
    "\n",
    "# TA = (['TA1','TA1','TA1',   # n=3xTA\n",
    "#        'TA2','TA2','TA2',\n",
    "#        'TA3','TA3','TA3',\n",
    "#        'TA4','TA4','TA4',\n",
    "#        'TA5','TA5','TA5',\n",
    "#        'TA6','TA6','TA6',\n",
    "#        'TA7','TA7','TA7',]) \n",
    "#TA = ['TA1','TA1','TA1'] # if want to test on only TA (smaller subset to go faster)\n",
    "#TA = ['TA1','TA2','TA3','TA4','TA5','TA6','TA7'] # n= 1xTA\n",
    "#TA = ['TA1'] # n=1xTA1\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "chunks = chunks\n",
    "seed = None \n",
    "temperature = 0.2\n",
    "model = \"llama-3.3-70b-instruct\" #\"gpt-4o\"\n",
    "\n",
    "date= '0218' # to indicate date in filenames\n",
    "output_directory = f'/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data/Outputs/{date}/'\n",
    "\n",
    "\n",
    "# (Loop tools)\n",
    "loop_counter = 0\n",
    "answers_content = []\n",
    "answers_metadata = pd.DataFrame(columns=[\"chunk\",  # or \"TA\"\n",
    "                                         \"replicate\",\n",
    "                                         \"model\",\n",
    "                                         \"seed\",\n",
    "                                         \"temperature\",\n",
    "                                         \"system_fingerprint\", \n",
    "                                         \"prompt_tokens\", \n",
    "                                         \"completion_tokens\"])  # create empty panda dataframe with the following columns so to gather a bit more data on the responses and ultimately try to assess consistency\n",
    "\n",
    "# Loop\n",
    "for x in range(len(targets_chunks)):\n",
    "\n",
    "    # Subset data to avoid overloading the GPT\n",
    "\n",
    "    # Subset per TA\n",
    "    #targets_subset = targets_pd[targets_pd['target_code'].str.contains(TA[x])]  # subset rows containing one of the characters in  TA[] (ie, select only a specific TA and its targets, because selecting everything in one go is too big for the AI to process)                                                                                                                                          # eg: TA[0] = 'TA1'\n",
    "    #targets_list = [f\"{row['target_code']}: {row['target_content']}\" for index, row in targets_subset.iterrows()] # Concatenate target_code and target_content into a list so that it can be added to the prompt as text\n",
    "    \n",
    "    # Subset per chunk\n",
    "    targets_subset = targets_chunks[x] \n",
    "    targets_list = [f\"{row['target_code']}: {row['target_content']}\" for index, row in targets_subset.iterrows()] # Concatenate target_code and target_content into a list so that it can be added to the prompt as text\n",
    "\n",
    "    \n",
    "    # Define request\n",
    "    prompt = f'''Hello, invent a short haiku based on this text: {targets_list}; '''\n",
    "\n",
    "\n",
    "\n",
    "    # Print to double check amount of tokens in prompt (JRC llama have a max of 120k)\n",
    "    print(f'''Chunk:{x+1}/{len(targets_chunks)} \\nTokens with o200k_base encoding: {num_tokens_from_string(prompt, \"o200k_base\")}\\nTokens with cl100k_base encoding: {num_tokens_from_string(prompt, \"cl100k_base\")}\\n ''')\n",
    "\n",
    "    # Generate answer\n",
    "    answer= get_chat_response(prompt=prompt,\n",
    "                              seed=seed,\n",
    "                              model=model,\n",
    "                              temperature=temperature  # The temperature parameter influences the randomness of the generated responses. A higher value, such as 0.8, makes the answers more diverse, while a lower value, like 0.2, makes them more focused and deterministic.\n",
    "                              )\n",
    "\n",
    "    answers_content.append((f'chunk{x+1}', answer['response_content'])) # add the different replicats for answers over a single TA in a same list so i can analyse the similarity later\n",
    "    answers_metadata.loc[x] = (f'{x+1}', #TA code\n",
    "                               f'', # replicate nbr\n",
    "                               model,  \n",
    "                               seed, \n",
    "                               temperature, \n",
    "                               answer[\"system_fingerprint\"],\n",
    "                               answer[\"prompt_tokens\"],\n",
    "                               answer[\"completion_tokens\"]\n",
    "                               ) \n",
    "    \n",
    "\n",
    "    \n",
    "    # Save response as csv file\n",
    "    #output_name = f'{date}output_{TA[x]}.{loop_counter+1}_s{seed}_t{temperature}.csv'    # -> to split by TA\n",
    "    output_name = f'{date}output_chk{x+1}_s{seed}_t{temperature}.csv'     # -> to split by chunks\n",
    "\n",
    "    with open((os.path.join(output_directory, output_name)), 'w') as f:\n",
    "         f.write(answer[\"response_content\"])\n",
    "\n",
    "    # (Extra loop tools)\n",
    "    loop_counter += 1         # incremental loop counter that resets to 0 every 3 loops so that it can add the \".1,2,3\" at the end of each triplicats file names\n",
    "    if loop_counter % 3 == 0:\n",
    "        loop_counter = 0\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Save triplicats metadata as csv\n",
    "#answers_metadata.to_csv(path_or_buf= f'{output_directory}{date}output_s{seed}_t{temperature}_metadata.csv', sep=';', index=False)  # split by TA method\n",
    "answers_metadata.to_csv(path_or_buf= f'{output_directory}{date}output_chk{len(targets_chunks)}_s{seed}_t{temperature}_metadata.csv', sep=';', index=False)  # split by chunks method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prompt = f'''Hello,\n",
    "\n",
    "#             Data input: get acquainted with the following data:\n",
    "#             - NACE classification categories:  {NACElevel1} + {NACElevel2} +{NACElevel3}.\n",
    "#             - List of targets: {target_list}.\n",
    "            \n",
    "#             Task: \n",
    "#             - For each target, analyse its content description and assign to each target a NACE category for each level (0,1,2,3). \n",
    "\n",
    "#             Answer format: provide your answer as a table in csv format please (separator: \";\"), with the following columns:\n",
    "#             - Target code (e.g., TA1.9)\n",
    "#             - Target content (e.g., The contribution of the sectors covered by the EU ETS with respect to the EU Climate ambition should be of -62 % compared to 2005 (increasing the linear emissions reduction factor from 2.2 % per year up to 4.4 %)) \n",
    "#             - NACE_level1 (e.g., D - Electricity, Gas, Steam and Air Conditioning Supply)\n",
    "#             - NACE_level1_extra1 (e.g.if other categories overlap)\n",
    "#             - NACE_level1_extra2 (e.g.if other categories overlap)\n",
    "#             - NACE_level2 (e.g., D35 - Electricity, gas, steam and air conditioning supply)\n",
    "#             - NACE_level2_extra1 (e.g.if other categories overlap)\n",
    "#             - NACE_level2_extra2 (e.g.if other categories overlap)\n",
    "#             - NACE_level_3 (e.g., D35.1 - Electric power generation, transmission and distribution)\n",
    "#             - NACE_level_3_extra1 (e.g. if other categories overlap)\n",
    "#             - NACE level3_extra2 (e.g. if other categories overlap)\n",
    "#             - Justification\n",
    "#             - Confidence (e.g. confidence value from 0 to 10 about the assignation choices that are made)\n",
    "\n",
    "#             Specifications:\n",
    "#             - If there is some overlap, add the multiple categories possibilites (up to maximum 3 per NACE level). \n",
    "#             - Include the name of the NACE categories.\n",
    "#             - Don't forget to provide the title of the target. If there is no target content, do not invent new content, just state it as empty.\n",
    "#             - For each target, write one or max 2 sentences justifying your choice.\n",
    "#             - Output only the csv table and no additional commentary text.\n",
    "\n",
    "#             Thank you.'''\n",
    "\n",
    "# # generate answer\n",
    "# answer= run_API_request(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(answer)\n",
    "\n",
    "#print answer in Markdown format\n",
    "display(Markdown(answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "file_name = 'estat_nama_10_a64_e'\n",
    "directory = f'/Users/giorgiobolchi2/Documents/JRC/LLM/Data/ESTAT/'\n",
    "\n",
    "\n",
    "tsv = pd.read_csv(f'{directory}{file_name}.tsv', sep='\\t')\n",
    "tsv.to_csv(f'{directory}{file_name}.csv', sep=';', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt= '''my script only requires these libraries: \n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pdfplumber\n",
    "import docx2txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    " \n",
    "from IPython.display import display, Markdown\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "import time\n",
    "import tiktoken\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdfplumber\n",
    "import sys\n",
    "import os\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pdfplumber\n",
    "import docx2txt \n",
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "but my requirements.txt file (created with pip freeze > requirements.txt) has much more things, can you sort out and give me back only the ones that are needed and i actually use? so that i can make a clean requirements.txt file: appnope==0.1.4\n",
    "asttokens==3.0.0\n",
    "attrs==24.3.0\n",
    "Automat==24.8.1\n",
    "beautifulsoup4==4.12.3\n",
    "bleach==6.2.0\n",
    "certifi==2024.12.14\n",
    "cffi==1.17.1\n",
    "charset-normalizer==3.4.1\n",
    "comm==0.2.2\n",
    "constantly==23.10.4\n",
    "contourpy==1.3.1\n",
    "cryptography==44.0.0\n",
    "cssselect==1.2.0\n",
    "cycler==0.12.1\n",
    "debugpy==1.8.9\n",
    "decorator==5.1.1\n",
    "defusedxml==0.7.1\n",
    "distlib==0.3.9\n",
    "et_xmlfile==2.0.0\n",
    "executing==2.1.0\n",
    "filelock==3.16.1\n",
    "fonttools==4.55.3\n",
    "fsspec==2024.12.0\n",
    "html2text==2024.2.26\n",
    "huggingface-hub==0.27.1\n",
    "hyperlink==21.0.0\n",
    "idna==3.10\n",
    "incremental==24.7.2\n",
    "ipykernel==6.29.5\n",
    "ipython==8.30.0\n",
    "itemadapter==0.10.0\n",
    "itemloaders==1.3.2\n",
    "jedi==0.19.2\n",
    "jmespath==1.0.1\n",
    "jupyter_client==8.6.3\n",
    "jupyter_core==5.7.2\n",
    "kiwisolver==1.4.8\n",
    "lxml==5.3.0\n",
    "Markdown==3.7\n",
    "matplotlib==3.10.0\n",
    "matplotlib-inline==0.1.7\n",
    "nest-asyncio==1.6.0\n",
    "networkx==3.4.2\n",
    "numpy==2.1.3\n",
    "Obsidian==1.2\n",
    "obsidiantools==0.10.0\n",
    "openpyxl==3.1.5\n",
    "packaging==24.2\n",
    "pandas==2.2.3\n",
    "parsel==1.9.1\n",
    "parso==0.8.4\n",
    "pexpect==4.9.0\n",
    "pillow==11.1.0\n",
    "platformdirs==4.3.6\n",
    "prompt_toolkit==3.0.48\n",
    "Protego==0.3.1\n",
    "psutil==6.1.0\n",
    "ptyprocess==0.7.0\n",
    "pure_eval==0.2.3\n",
    "pyasn1==0.6.1\n",
    "pyasn1_modules==0.4.1\n",
    "pycparser==2.22\n",
    "PyDispatcher==2.0.7\n",
    "Pygments==2.18.0\n",
    "pymdown-extensions==10.14\n",
    "pyOpenSSL==25.0.0\n",
    "pyparsing==3.2.1\n",
    "python-dateutil==2.9.0.post0\n",
    "python-frontmatter==1.1.0\n",
    "pytz==2024.2\n",
    "PyYAML==6.0.2\n",
    "pyzmq==26.2.0\n",
    "queuelib==1.7.0\n",
    "regex==2024.11.6\n",
    "requests==2.32.3\n",
    "requests-file==2.1.0\n",
    "safetensors==0.5.2\n",
    "Scrapy==2.12.0\n",
    "service-identity==24.2.0\n",
    "setuptools==75.8.0\n",
    "six==1.17.0\n",
    "soupsieve==2.6\n",
    "stack-data==0.6.3\n",
    "tabulate==0.9.0\n",
    "tldextract==5.1.3\n",
    "tokenizers==0.21.0\n",
    "tornado==6.4.2\n",
    "tqdm==4.67.1\n",
    "traitlets==5.14.3\n",
    "transformers==4.48.0\n",
    "Twisted==24.11.0\n",
    "typing_extensions==4.12.2\n",
    "tzdata==2024.2\n",
    "urllib3==2.3.0\n",
    "virtualenv==20.29.1\n",
    "w3lib==2.2.1\n",
    "wcwidth==0.2.13\n",
    "webencodings==0.5.1\n",
    "zope.interface==7.2\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the libraries you've listed as required, I've filtered out the unnecessary ones from your `requirements.txt` file. Here are the required libraries:\n",
      "\n",
      "```\n",
      "networkx==3.4.2\n",
      "numpy==2.1.3\n",
      "pandas==2.2.3\n",
      "pdfplumber\n",
      "docx2txt\n",
      "openai\n",
      "matplotlib==3.10.0\n",
      "requests==2.32.3\n",
      "tiktoken\n",
      "```\n",
      "\n",
      "However, please note that some of these libraries may have dependencies that are not explicitly listed here. For example, `openai` may require `requests` and other libraries to function properly.\n",
      "\n",
      "To ensure that all dependencies are included, you can use the following command to generate a `requirements.txt` file:\n",
      "\n",
      "```bash\n",
      "pip freeze > requirements.txt\n",
      "```\n",
      "\n",
      "Then, manually remove the unnecessary libraries from the file.\n",
      "\n",
      "Alternatively, you can use a virtual environment to manage your dependencies. Here's an example of how to create a virtual environment and install the required libraries:\n",
      "\n",
      "```bash\n",
      "# Create a virtual environment\n",
      "python -m venv myenv\n",
      "\n",
      "# Activate the virtual environment\n",
      "source myenv/bin/activate  # On Linux/Mac\n",
      "myenv\\Scripts\\activate  # On Windows\n",
      "\n",
      "# Install the required libraries\n",
      "pip install networkx numpy pandas pdfplumber docx2txt openai matplotlib requests tiktoken\n",
      "\n",
      "# Generate a requirements.txt file\n",
      "pip freeze > requirements.txt\n",
      "```\n",
      "\n",
      "This way, you can ensure that all dependencies are included in the `requirements.txt` file, and you can easily manage your virtual environment.\n"
     ]
    }
   ],
   "source": [
    "# QUICK ANSWER \n",
    "\n",
    "# Define absolute python path\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/') \n",
    "\n",
    "from Code.API import get_chat_response\n",
    "\n",
    "\n",
    "seed = None \n",
    "temperature = 0.1\n",
    "model = \"llama-3.3-70b-instruct\" \n",
    "prompt = prompt\n",
    "\n",
    "\n",
    "answer= get_chat_response(prompt=prompt,\n",
    "                              seed=seed,\n",
    "                              model=model,\n",
    "                              temperature=temperature\n",
    "                              )\n",
    "\n",
    "\n",
    "print(answer['response_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
