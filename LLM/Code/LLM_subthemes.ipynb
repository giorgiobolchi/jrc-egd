{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pdfplumber\n",
    "import docx2txt\n",
    "\n",
    "\n",
    "# Define absolute python path\n",
    "sys.path.insert(0, '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/') \n",
    "\n",
    "\n",
    "## FUNCTIONS\n",
    "\n",
    "# Load API and import request function\n",
    "from Code.API import get_chat_response, num_tokens_from_string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "\n",
    "target_data_directory = '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data'\n",
    "report1_directory = '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data/REPORT_1/report1_extra-trimmed.pdf'\n",
    "report2_directory = '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data/REPORT_2/access_20250310'\n",
    "\n",
    "# Import all target data (target_code + target_content)\n",
    "target_data_250 = pd.read_csv(f'{target_data_directory}/targets_data_254.csv', sep=\";\")  #extensive target list from target_NACE_classification.xlsx\n",
    "target_data_150 = pd.read_csv(f'{target_data_directory}/targets_data_150.csv', sep=\";\")  #target list as in report 1 + assessments\n",
    "\n",
    "\n",
    "# Import & parse report1\n",
    "with pdfplumber.open(report1_directory) as pdf:\n",
    "    # Extract the text from the PDF\n",
    "    report1 = \"\"\n",
    "    for page in pdf.pages:\n",
    "        report1 += page.extract_text()\n",
    "\n",
    "# Clean-up report1\n",
    "report1 = report1.strip() \n",
    "report1 = report1.replace(\"\\n\", \" \")\n",
    "report1 = report1.replace(\"\\t\", \" \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List thematic areas\n",
    "thematic_areas = {\n",
    "    'TA1': 'TA1_Climate ambition',\n",
    "    'TA2': 'TA2_Clean, affordable and secure energy',\n",
    "    'TA3': 'TA3_Industrial strategy for a clean and circular economy',\n",
    "    'TA4': 'TA4_Sustainable and smart mobility',\n",
    "    'TA5': 'TA5_Greening the Common Agricultural Policy - Farm to Fork Strategy',\n",
    "    'TA6': 'TA6_Preserving and protecting biodiversity',\n",
    "    'TA7': 'TA7_Towards a zero-pollution ambition for a toxic free environment',\n",
    "}\n",
    "\n",
    "\n",
    "# Sub-themes determined manually in Obsidian Canvases (building up on sub-themes used in report 1)\n",
    "sub_themes = [\n",
    "\t\n",
    "\t\"Climate Resilience\",\n",
    "\t\"GHG Reduction\",\n",
    "\t\"GHG Reduction - Buildings\",\n",
    "\t\"GHG Reduction - Transports\"\n",
    "\t\"GHG Removal\",\n",
    "\t\"Renewable Energy\",\n",
    "\t\"Renewable Energy - Heating & Cooling\",\n",
    "\t\"Renewable Energy - Hydrogen Production\",\n",
    "\t\"Renewable Energy - Ocean/Offshore\",\n",
    "\t\"Renewable Energy - Solar\",\n",
    "\t\"Energy Efficiency\",\n",
    "\t\"Energy Efficiency - Buildings\",\n",
    "\t\"Energy Infrastructure\",\n",
    "\t\"Methane\",\n",
    "\t\"Social Security - Energy\",\n",
    "\t\"Waste Reduction\",\n",
    "\t\"Waste Reduction - Municipal Waste\",\n",
    "\t\"Waste Reduction - Food Waste\",\n",
    "\t\"Waste Reduction - Plastic & Packaging\",\n",
    "\t\"Circularity/Recycling\",\n",
    "\t\"Circularity/Recycling - Municipal Waste\",\n",
    "\t\"Circularity/Recycling - Textile Waste\",\n",
    "\t\"Circularity/Recycling - Plastic & Packaging\",\n",
    "\t\"Circularity/Recycling - Plastic & Packaging - Bio-based plastics\",\n",
    "\t\"Circularity/Recycling - Vehicle Circularity\",\n",
    "\t\"Circularity/Recycling - Critical Raw Materials - Batteries Recycling\",\n",
    "\t\"Critical Raw Materials - Extraction & Import\",\n",
    "\t\"Net-Zero Technology - Manufacturing\",\n",
    "\t\"Rail\",\n",
    "\t\"Net-Zero Technology - Road Vehicles\",\n",
    "\t\"Net-Zero Technology - Maritime Transport\",\n",
    "\t\"Net-Zero Technology - Aviation\",\n",
    "\t\"Biofuels\",\n",
    "\t\"Other Low-Carbon Fuels\",\n",
    "\t\"Hydrogen Distribution\",\n",
    "\t\"Urban Mobility\",\n",
    "\t\"Transport Logistics\",\n",
    "\t\"Food quality\",\n",
    "\t\"Food quality - Animal Welfare\",\n",
    "\t\"Food quality - Healthy Food\",\n",
    "\t\"Food affordability\",\n",
    "\t\"Pesticides Reduction\",\n",
    "\t\"Competitive Agriculture\",\n",
    "\t\"Social Security - Workers Protection\",\n",
    "\t\"Terrestrial Ecosystems Restoration\",\n",
    "\t\"Terrestrial Ecosystems Restoration - Rivers\",\n",
    "\t\"Terrestrial Ecosystems Restoration - Agricultural Ecosystems\",\n",
    "\t\"Terrestrial Ecosystems Restoration - Forests\",\n",
    "\t\"Marine Ecosystem Restoration\",\n",
    "\t\"Biodiversity Protection & Conservation\",\n",
    "\t\"Biodiversity Protection & Conservation - Fisheries\",\n",
    "\t\"Biodiversity Protection & Conservation - Monitoring\",\n",
    "\t\"Biodiversity Protection & Conservation - Urban Nature\",\n",
    "\t\"Forest Bioeconomy\",\n",
    "\t\"Improve Air Quality\",\n",
    "\t\"Improve Water Quality\",\n",
    "\t\"Improve Soils Health\",\n",
    "\t\"Noise Reduction\",\n",
    "\t\"Social Security - Sanitation\"\n",
    "\t\n",
    "\t]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE ANSWERS\n",
    "\n",
    "# Set parameters\n",
    "chunks = 3\n",
    "target_data = target_data_250\n",
    "seed = None\n",
    "temperature = 0.1\n",
    "model = \"llama-3.3-70b-instruct\"  # \"llama-3.3-70b-instruct\" \"gpt-4o\"\n",
    "date = '0325'\n",
    "output_dir = f'/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data/Outputs/{date}/'\n",
    "os.makedirs(output_dir, exist_ok=True)  # Create the 'date' folder if it doesn't exist\n",
    "\n",
    "# Chunk generation\n",
    "#target_data = target_data.sample(frac=1, random_state=1).reset_index()  # shuffle dataframe rows IF NECESSARY\n",
    "target_data_chunks = np.array_split(ary=target_data, indices_or_sections=chunks)  # generate chunks\n",
    "\n",
    "for x in range(len(target_data_chunks)):\n",
    "#for x in list([9]):\n",
    "    success = False  # Initialize a flag to track whether the operation was successful\n",
    "    retry_count = 0  # Initialize a counter to track the number of retries\n",
    "    max_retries = 3  # adjust this value to set the desired number of retries\n",
    "\n",
    "    while not success and retry_count < max_retries:\n",
    "        try:\n",
    "            target_list = [f\"{row['target_code']}: {row['target_content']}\" for index, row in target_data_chunks[x].iterrows()]  # Concatenate target_code and target_content into a list so that it can be added to the prompt as text string\n",
    "\n",
    "            # Define request\n",
    "            prompt = f'''\n",
    "            Data input & Context:\n",
    "            - List of thematic areas (TA): {thematic_areas}\n",
    "            - List of European Green Deal (EGD) targets: {target_list}.\n",
    "            - List of sub-themes: {sub_themes}.\n",
    "\n",
    "            Task: \n",
    "            - For each target, assign a sub-theme from the list based on the target content.\n",
    "\n",
    "            Answer format: provide your answer as a table in csv format (separator: \";\"), with the following columns:\n",
    "            - target_code (e.g., TA2.7)\n",
    "            - target_content (e.g., Over this decade, the EU will need to install, on average, approximately 45 GW per year of PV to reach the share of 45% of energy coming from renewables set out in the REPowerEU Plan.) \n",
    "            - thematic_area (e.g., TA2_Clean, affordable and secure energy)\n",
    "            - sub_theme (e.g., Renewable Energy - Solar)\n",
    "            - justification\n",
    "\n",
    "            Specifications:\n",
    "            - This is crucial: all targets should be assigned to at least one sub-theme.\n",
    "            - If one target is fitting in multiple sub-themes, add multiple rows for one same target.\n",
    "            - If you cannot classify one target to any sub-theme, assign it to a sub-theme called 'Unclassified'. \n",
    "            - For each target, write 1-2 sentences to justify why a particular target was assigned to a specific sub-theme.\n",
    "            - Output only the CSV table. Do not include additional commentary.\n",
    "            \n",
    "            '''\n",
    "\n",
    "            # Print to double check amount of tokens in prompt (JRC llama should have a max of 120k)\n",
    "            prompt_tokens = f'''Prompt length: {len(prompt)} \\nPrompt tokens (o200k_base encoding): {num_tokens_from_string(prompt, \"o200k_base\")} \\nPrompt tokens (cl100k_base encoding): {num_tokens_from_string(prompt, \"cl100k_base\")} \\n'''\n",
    "            print(prompt_tokens)\n",
    "\n",
    "            # Generate answer\n",
    "            answer = get_chat_response(prompt=prompt,\n",
    "                                      seed=seed,\n",
    "                                      model=model,\n",
    "                                      temperature=temperature  # The temperature parameter influences the randomness of the generated responses. A higher value, such as 0.8, makes the answers more diverse, while a lower value, like 0.2, makes them more focused and deterministic.\n",
    "                                      )\n",
    "\n",
    "            # Save the generated answer as a CSV file\n",
    "            output_name = f'{date}_subthemes_chunk{x+1}.csv'\n",
    "\n",
    "            with open((os.path.join(output_directory, output_name)), 'w') as f:\n",
    "                f.write(answer[\"response_content\"])\n",
    "\n",
    "            # If success, set the success flag to True\n",
    "            success = True\n",
    "\n",
    "            # If success, add a 2-minute pause between answer requests to avoid RateLimitErrors\n",
    "            print(f\"-- 1 min pause \\n\")\n",
    "            time.sleep(60)\n",
    "\n",
    "        except Exception as e:\n",
    "            retry_count += 1  # Increment the retry counter if an error occurs\n",
    "            error_type = type(e).__name__  # Get the type of error that occurred\n",
    "            error_message = str(e)  # Get the error message\n",
    "            print(f\"An error occurred ({error_type}): {error_message}. Retrying ({retry_count}/{max_retries})\")  # Print an error message with the type and message\n",
    "\n",
    "    # Print a message if the operation failed after the maximum number of retries\n",
    "    if not success:\n",
    "        print(f\"Failed to generate answer for pair{x} after {max_retries} retries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results chunks into a single file\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Specify the directory path and file pattern\n",
    "date = '0319'\n",
    "output_dir = output_dir\n",
    "file_pattern = f'{date}_subthemes_chunk*.csv'\n",
    "\n",
    "# Get a list of all CSV files matching the pattern\n",
    "csv_files = glob.glob(output_directory + '/' + file_pattern)\n",
    "\n",
    "# Initialize an empty list to store the dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Iterate over each CSV file, read it into a dataframe, and append to the list\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, on_bad_lines='skip', sep=';')\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file}: {e}\")\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "if dataframes:\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "else:\n",
    "    print(\"No dataframes to concatenate.\")\n",
    "\n",
    "# Write the combined dataframe to a new CSV file\n",
    "combined_df.to_csv(f'{output_directory}{date}_subthemes.csv', index=True, sep=';')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
