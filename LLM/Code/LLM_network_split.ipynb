{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pdfplumber\n",
    "import docx2txt\n",
    "\n",
    "\n",
    "# Define absolute python path\n",
    "sys.path.insert(0, '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/') \n",
    "\n",
    "\n",
    "## FUNCTIONS\n",
    "\n",
    "# Load API and import request function\n",
    "from Code.API import get_chat_response, num_tokens_from_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE DIRECTORIES\n",
    "\n",
    "# Define the project root directory\n",
    "root_dir = '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/'\n",
    "\n",
    "# File paths\n",
    "report1_dir = '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data/REPORT_1/report1_trimmed.pdf'\n",
    "report2_dir = '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data/REPORT_2/access_20250310'\n",
    "target_data_dir  = '/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT DATA\n",
    "\n",
    "# Import all target data (target_code + target_content)\n",
    "target_data_250 = pd.read_csv(f'{target_data_dir}/targets_data_250.csv', sep=\";\")  #extensive target list from target_NACE_classification.xlsx\n",
    "target_data_150 = pd.read_csv(f'{target_data_dir}/targets_data_150.csv', sep=\";\")  #target list as in report 1\n",
    "\n",
    "\n",
    "# Import & parse report1\n",
    "with pdfplumber.open(report1_dir) as pdf:\n",
    "    # Extract the text from the PDF\n",
    "    report1 = \"\"\n",
    "    for page in pdf.pages:\n",
    "        report1 += page.extract_text()\n",
    "\n",
    "# Clean-up report1\n",
    "report1 = report1.strip() # remove leading and trailing whitespaces\n",
    "report1 = report1.replace(\"\\n\", \" \") # remove newlines\n",
    "report1 = report1.replace(\"\\t\", \" \") # remove tabs\n",
    "\n",
    "# Import & parse report2\n",
    "\n",
    "# create a dictionary to access different chapters of report2   \n",
    "report2 = { \n",
    "    'chapter1': docx2txt.process(f'{report2_dir}/NEW_Chapter1_CLEAN - Introduction & setting the scene_LM_trimmed.docx'), # extract text from docx files\n",
    "    'chapter2': docx2txt.process(f'{report2_dir}/NEW_Chapter2 (ex chp3) - Environmental impacts_ZOTERO_trimmed.docx'),\n",
    "    'chapter3': docx2txt.process(f'{report2_dir}/NEW_Chapter3 (ex chp4) with BIBLIO - Challenges and enablers for EGD objectives_trimmed.docx'),\n",
    "    'chapter4': docx2txt.process(f'{report2_dir}/NEW_Chapter4 (ex chp5) - Enabling the green transition_trimmed.docx'),\n",
    "    'chapter5': docx2txt.process(f'{report2_dir}/NEW_Chapter5 - Fair and just transition_trimmed.docx'),\n",
    "    'chapter6': docx2txt.process(f'{report2_dir}/NEW_Chapter6 - Financing the green transition.docx')\n",
    "}\n",
    "\n",
    "# clean up report2 chapters\n",
    "for chapter, text in report2.items():\n",
    "    report2[chapter] = text.strip()\n",
    "    report2[chapter] = report2[chapter].replace(\"\\n\", \" \")\n",
    "    report2[chapter] = report2[chapter].replace(\"\\t\", \" \")\n",
    "\n",
    "\n",
    "\n",
    "subthemes_list = {        # a manually-selected list of sub-themes per thematic area.\n",
    "    'TA1': [\n",
    "        \"Climate Resilience\",\n",
    "        \"GHG Reduction\",\n",
    "        \"GHG Reduction - Buildings\",\n",
    "        \"GHG Reduction - Transports\"\n",
    "        \"GHG Removal\",\n",
    "    ],\n",
    "    'TA2': [\n",
    "        \"Renewable Energy\",\n",
    "        \"Renewable Energy - Heating & Cooling\",\n",
    "        \"Renewable Energy - Hydrogen Production\",\n",
    "        \"Renewable Energy - Ocean/Offshore\",\n",
    "        \"Renewable Energy - Solar\",\n",
    "        \"Energy Efficiency\",\n",
    "        \"Energy Efficiency - Buildings\",\n",
    "        \"Energy Infrastructure\",\n",
    "        \"Social Security - Energy\",\n",
    "    ],\n",
    "    'TA3': [\n",
    "        \t\"Waste Reduction\",\n",
    "          \"Waste Reduction - Municipal Waste\",\n",
    "          \"Waste Reduction - Food Waste\",\n",
    "          \"Waste Reduction - Plastic & Packaging\",\n",
    "          \"Circularity/Recycling\",\n",
    "          \"Circularity/Recycling - Municipal Waste\",\n",
    "          \"Circularity/Recycling - Textile Waste\",\n",
    "          \"Circularity/Recycling - Plastic & Packaging\",\n",
    "          \"Circularity/Recycling - Plastic & Packaging - Bio-based plastics\",\n",
    "          \"Circularity/Recycling - Vehicle Circularity\",\n",
    "          \"Circularity/Recycling - Critical Raw Materials - Batteries Recycling\",\n",
    "          \"Critical Raw Materials - Extraction & Import\",\n",
    "          \"Net-Zero Technology - Manufacturing\",\n",
    "    ],\n",
    "    'TA4': [\n",
    "        \t\"Rail\",\n",
    "          \"Net-Zero Technology - Road Vehicles\",\n",
    "          \"Net-Zero Technology - Maritime Transport\",\n",
    "          \"Net-Zero Technology - Aviation\",\n",
    "          \"Biofuels\",\n",
    "          \"Other Low-Carbon Fuels\",\n",
    "          \"Hydrogen Distribution\",\n",
    "          \"Urban Mobility\",\n",
    "          \"Transport Logistics\",\n",
    "    ],\n",
    "    'TA5': [\n",
    "        \t\"Food quality\",\n",
    "          \"Food quality - Animal Welfare\",\n",
    "          \"Food quality - Healthy Food\",\n",
    "          \"Food affordability\",\n",
    "          \"Pesticides Reduction\",\n",
    "          \"Competitive Agriculture\",\n",
    "          \"Social Security - Workers Protection\",\n",
    "    ],\n",
    "    'TA6': [\n",
    "        \t\"Terrestrial Ecosystems Restoration\",\n",
    "          \"Terrestrial Ecosystems Restoration - Rivers\",\n",
    "          \"Terrestrial Ecosystems Restoration - Agricultural Ecosystems\",\n",
    "          \"Terrestrial Ecosystems Restoration - Forests\",\n",
    "          \"Marine Ecosystem Restoration\",\n",
    "          \"Biodiversity Protection & Conservation\",\n",
    "          \"Biodiversity Protection & Conservation - Fisheries\",\n",
    "          \"Biodiversity Protection & Conservation - Monitoring\",\n",
    "          \"Biodiversity Protection & Conservation - Urban Nature\",\n",
    "    ],\n",
    "    'TA7': [\n",
    "        \"Forest Bioeconomy\",\n",
    "        \"Improve Air Quality\",\n",
    "        \"Improve Water Quality\",\n",
    "        \"Improve Soils Health\",\n",
    "        \"Noise Reduction\",\n",
    "        \"Social Security - Sanitation\"\n",
    "          ],\n",
    "}\n",
    "\n",
    "\n",
    "impact_weight_meanings = {     # a list in json object format so that the LLM can most efficiently understand its structure, based on Nilssen et al. (2016)\n",
    "  \"weights\": [\n",
    "    {\n",
    "      \"weight\": \"+3\",\n",
    "      \"name\": \"Indivisible\",\n",
    "      \"explanation\": \"Inextricably linked to the achievement of another target.\",\n",
    "      \"example\": \"Ending all forms of discrimination against women and girls is indivisible from ensuring women’s full and effective participation and equal opportunities for leadership.\"\n",
    "    },\n",
    "    {\n",
    "      \"weight\": \"+2\",\n",
    "      \"name\": \"Reinforcing\",\n",
    "      \"explanation\": \"Aids the achievement of another target.\",\n",
    "      \"example\": \"Providing access to electricity reinforces water‐pumping and irrigation systems. Strengthening the capacity to adapt to climate‐related hazards reduces losses caused by disasters.\"\n",
    "    },\n",
    "    {\n",
    "      \"weight\": \"+1\",\n",
    "      \"name\": \"Enabling\",\n",
    "      \"explanation\": \"Creates conditions that further another target.\",\n",
    "      \"example\": \"Providing electricity access in rural homes enables education, because it makes it possible to do homework at night with electric lighting.\"\n",
    "    },\n",
    "    {\n",
    "      \"weight\": \"-1\",\n",
    "      \"name\": \"Constraining\",\n",
    "      \"explanation\": \"Limits options on another target.\",\n",
    "      \"example\": \"Improved water efficiency can constrain agricultural irrigation. Reducing climate change can constrain the options for energy access.\"\n",
    "    },\n",
    "    {\n",
    "      \"weight\": \"-2\",\n",
    "      \"name\": \"Counteracting\",\n",
    "      \"explanation\": \"Clashes with another target.\",\n",
    "      \"example\": \"Boosting consumption for growth can counteract waste reduction and climate mitigation.\"\n",
    "    },\n",
    "    {\n",
    "      \"weight\": \"-3\",\n",
    "      \"name\": \"Cancelling\",\n",
    "      \"explanation\": \"Makes it impossible to reach another goal.\",\n",
    "      \"example\": \"Fully ensuring public transparency and democratic accountability cannot be combined with national‐security goals. Full protection of natural reserves excludes public access for recreation.\"\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SELECT DATA & DEFINE LLM PARAMETERS\n",
    "\n",
    "# LLM parameters\n",
    "data = target_data_150\n",
    "seed = None \n",
    "temperature = 0.1\n",
    "model = \"llama-3.3-70b-instruct\" #\"llama-3.3-70b-instruct\" \"gpt-4o\" \"nous-hermes-2-mixtral-8x7b-dpo\"\n",
    "date = '0328' \n",
    "output_dir = f'/Users/giorgiobolchi2/Documents/GitHub/jrc-egd/LLM/Data/Outputs/{date}/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE DATA DICTIONARY\n",
    "\n",
    "# This code block generates a nested dictionary (target_data_dict) to organize target data by thematic area code and sub-theme. \n",
    "# The resulting dictionary has the following structure: 'target_data_dict' / thematic_area_code / sub_theme / ta_code | ta_content | ta_assessment.\n",
    "\n",
    "# Select the dataset to work with\n",
    "data = data \n",
    "\n",
    "# 1) Create a first sub-dictionary to store the subthemes and their respective target data\n",
    "subthemes_dict = {}\n",
    "\n",
    "for theme in data['sub_theme'].unique(): # Iterate over the unique 'sub_theme' values\n",
    "    theme_df = data[data['sub_theme'] == theme][['target_code', 'target_content', 'target_assessment']]     # Filter the DataFrame for the current 'sub_theme'\n",
    "    subthemes_dict[theme] = theme_df    # Add the filtered DataFrame to the dictionary\n",
    "\n",
    "# 2) Create the final overarching dictionary and sort into it the sub-themes and their target data per thematic areas\n",
    "target_data_dict = {}\n",
    "\n",
    "for ta in data['thematic_area_code'].unique(): # Iterate over the unique 'thematic_area_code' values\n",
    "    target_data_dict[ta] = {}    # Initialize the thematic area in the dictionary\n",
    "    for theme in data[data['thematic_area_code'] == ta]['sub_theme'].unique():     # Iterate over the unique 'sub_theme' values for the current thematic area\n",
    "        theme_df = subthemes_dict[theme] # Get the sub-theme DataFrame from the subthemes_dict\n",
    "        target_data_dict[ta][theme] = theme_df # Add the sub-theme DataFrame to the target_data_150_dict\n",
    "\n",
    "\n",
    "# example: \n",
    "# print(target_data_dict['TA3']['Circularity/Recycling - Vehicle Circularity']) # example of how to access a specific sub-theme in the dictionary (for testing purposes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TA PAIRS GENERATION \n",
    "\n",
    "# Generate all potential pairs of thematic areas (2x21= 42 pairs)\n",
    "\n",
    "ta_pairs = {} # dictionary to store all thematic area pairs\n",
    "ta_list = ['TA1', 'TA2', 'TA3', 'TA4', 'TA5', 'TA6', 'TA7'] # list of thematic areas\n",
    "pair_id = 0 \n",
    "\n",
    "for i in range(len(ta_list)):\n",
    "    for j in range(len(ta_list)):\n",
    "        if i != j:\n",
    "            ta_pairs[pair_id] = [ta_list[i], ta_list[j]] \n",
    "            pair_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE ANSWERS\n",
    "\n",
    "# Inputs:\n",
    "#     ta_pairs: a list of pairs of thematic area codes\n",
    "#     target_data_dict: a dictionary containing target data for each thematic area code\n",
    "#     report2: a dictionary containing chapters of report 2\n",
    "#     subthemes_list: a list of sub-themes for each thematic area code\n",
    "#     impact_weight_meanings: a dictionary containing meanings for impact weights\n",
    "#     model, seed, temperature: LLM parameters\n",
    "\n",
    "# Outputs:\n",
    "#     A set of CSV files containing generated answers for each pair of thematic area codes, saved in the output_directory\n",
    "#     A CSV file containing metadata for the generated answers, saved in the output_directory as {date}_network_metadata.csv\n",
    "\n",
    "\n",
    "\n",
    "# (Loop tools and formatting)\n",
    "os.makedirs(output_dir, exist_ok=True) # Create the 'date' folder if it doesn't exist\n",
    "answers_metadata = pd.DataFrame(columns=[\"ta_pairs_nbr\",\n",
    "                                         \"ta_pairs_pairs\",        # create empty panda dataframe with the following columns so to gather a bit more data on the responses and ultimately try to assess consistency\n",
    "                                         \"model\",\n",
    "                                         \"seed\",\n",
    "                                         \"temperature\",\n",
    "                                         \"system_fingerprint\", \n",
    "                                         \"prompt_tokens\", \n",
    "                                         \"completion_tokens\"])  \n",
    "\n",
    "\n",
    "# Loop\n",
    "\n",
    "for x in range(len(ta_pairs)):\n",
    "#for x in range(8, 42):     # for testing purposes\n",
    "#for x in list([12,13,14]): # for testing purposes\n",
    "\n",
    "    success = False  # Initialize a flag to track whether the operation was successful\n",
    "    retry_count = 0  # Initialize a counter to track the number of retries\n",
    "    max_retries = 5  # adjust this value to set the desired number of retries\n",
    "\n",
    "    while not success and retry_count < max_retries:\n",
    "        try:\n",
    "            # Subset data to avoid overloading the model\n",
    "            sub1 = [f\"{target_data_dict[ta_pairs[x][0]]}\"] # this will access the data stored in target_data_dict for the thematic_area_code stored in ta_pairs[x][0] (e.g., target_data_dict[ta_pairs[0][0]] <=> target_data_dict['TA1])\n",
    "            sub2 = [f\"{target_data_dict[ta_pairs[x][1]]}\"] # same thing here, but for the second element of the pair.\n",
    "\n",
    "\n",
    "            # Define prompt\n",
    "            prompt = f'''\n",
    "                Data input & Context:\n",
    "                - List A: first list of European Green Deal (EGD) targets grouped by sub-themes:{sub1}.\n",
    "                - List B: second list of EGD targets grouped by sub-themes: {sub2}.\n",
    "                - Report n°2: [ {report2['chapter1']} + {report2['chapter2']} + {report2['chapter3']} + {report2['chapter4']} + {report2['chapter5']} ].\n",
    "\n",
    "                Task: \n",
    "                - Determine how and how much sub-themes in List A may positiviely or negatively influence sub-themes in List B (i.e., determine potential synergies and/or trade-offs).\n",
    "                - Take into account the context and information of Report n°2 as well as the information available about the targets in both lists.\n",
    "\n",
    "                Answer format: provide your answer as a table in csv format please (separator: \";\"), with the following columns:\n",
    "                - source_subtheme (e.g., GHG Reduction).\n",
    "                - source_subtheme_targets (e.g.,TA1.3,TA1.7,TA1.9,TA1.11,TA1.13,TA5.7) .\n",
    "                - impact_subtheme (the name of the subtheme that is likely to be positively or negatively affected by the implementation and requirements of the sub-theme in the 'source_subtheme' column).\n",
    "                - impact_type (positive '+' or negative '-').\n",
    "                - impact_weight (-3,-2,-1,1,2,3).\n",
    "                - justification.\n",
    "\n",
    "                Specifications:\n",
    "                - The impacts can have different weights, which have the following meanings: {impact_weight_meanings}\n",
    "                - Only the following sub-themes can be added to the table: {subthemes_list[ta_pairs[x][0]]} and {subthemes_list[ta_pairs[x][1]]}.\n",
    "                - This is crucial: do not invent new sub-themes.\n",
    "                - Connections can only be made from sub-themes in List A to sub-themes in List B, not the contrary.\n",
    "                - If some sub-themes do not have any connections at all (i.e., are isolated), do not add any row.\n",
    "                - One row per connection, if you deem that one sub-theme has an impact on multiple other sub-themes, add as many rows for a same sub-theme as necessary.\n",
    "                - It is critical that your analysis is based on the context of the report and not just on the semantics of the target contents.\n",
    "                - This is mandatory: for each sub-theme connection, write 1-2 concise sentences justifying your choice. \n",
    "                - Output only the CSV table. Do not include additional commentary.\n",
    "            '''\n",
    "\n",
    "            # Print pre-generation metadata (to double check amount of tokens in prompt, JRC llama3.3 should have a max of 120k)\n",
    "            prompt_metadata = f'''TA_pair: {x} - {ta_pairs[x]} \\nPrompt length: {len(prompt)} \\nPrompt tokens (o200k_base encoding): {num_tokens_from_string(prompt, \"o200k_base\")} \\nPrompt tokens (cl100k_base encoding): {num_tokens_from_string(prompt, \"cl100k_base\")} \\n'''\n",
    "            print(prompt_metadata)\n",
    "\n",
    "            # Generate answer\n",
    "            answer = get_chat_response(prompt=prompt,\n",
    "                                      seed=seed,\n",
    "                                      model=model,\n",
    "                                      temperature=temperature)\n",
    "\n",
    "            # Print post-generation metadata \n",
    "            print(f'Answer generated.')\n",
    "            print(f'Prompt tokens: {answer[\"prompt_tokens\"]} \\nCompletion tokens: {answer[\"completion_tokens\"]}')\n",
    "\n",
    "\n",
    "            # Add the metadata of the generated answer to a dataframe\n",
    "            answers_metadata.loc[x] = (x,\n",
    "                                       ta_pairs[x],\n",
    "                                       model,\n",
    "                                       seed,\n",
    "                                       temperature,\n",
    "                                       answer[\"system_fingerprint\"],\n",
    "                                       answer[\"prompt_tokens\"],\n",
    "                                       answer[\"completion_tokens\"])\n",
    "\n",
    "            # Save the generated answer as a CSV file\n",
    "            output_name = f'{date}_network_pair{x}.csv'\n",
    "\n",
    "            with open((os.path.join(output_dir, output_name)), 'w') as f:\n",
    "                f.write(answer[\"response_content\"])\n",
    "\n",
    "            # If success, set the success flag to True\n",
    "            success = True\n",
    "\n",
    "            # If success, add a 2-minute pause between answer requests to avoid RateLimitErrors\n",
    "            print(f\"-- 1 min pause \\n\")\n",
    "            time.sleep(60)\n",
    "\n",
    "        except Exception as e:\n",
    "            \n",
    "            retry_count += 1  # Increment the retry counter if an error occurs\n",
    "            error_type = type(e).__name__  # Get the type of error that occurred\n",
    "            error_message = str(e) # Get the error message\n",
    "            print(f\"An error occurred ({error_type}): {error_message}. Retrying ({retry_count}/{max_retries})\")  # Print an error message with the type and message\n",
    "\n",
    "\n",
    "    # Print a message if the operation failed after the maximum number of retries\n",
    "    if not success:\n",
    "        print(f\"Failed to generate answer for pair{x} ({ta_pairs[x]}) after {max_retries} retries.\")\n",
    "\n",
    "# Save the metadata dataframe as a CSV file\n",
    "answers_metadata.to_csv(path_or_buf=os.path.join(output_dir, f'{date}_network_metadata.csv'),\n",
    "                         sep=';',\n",
    "                         index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULTS AGGREGATION\n",
    "\n",
    "# This code aggregates the results from multiple CSV files generated by the previous block into a single dataframe. \n",
    "# The combined dataframe is then written to a new CSV file named {date}_network_aggregated.csv in the specified output directory.\n",
    "\n",
    "\n",
    "# Specify the directory path and file pattern\n",
    "date = date\n",
    "output_dir = output_dir\n",
    "file_pattern = f'{date}_network_pair*.csv' # pattern to match all files generated in the current session\n",
    "\n",
    "csv_files = glob.glob(output_dir + '/' + file_pattern) # Get a list of all CSV files matching the pattern\n",
    "\n",
    "# Initialize an empty list to store the dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Iterate over each CSV file, read it into a dataframe, and append to the list\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, on_bad_lines='skip', sep=';')\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file}: {e}\")\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "if dataframes:\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "else:\n",
    "    print(\"No dataframes to concatenate.\")\n",
    "\n",
    "# Write the combined dataframe to a new CSV file\n",
    "combined_df.to_csv(f'{output_dir}{date}_network_aggregated.csv', index=True, sep=';')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
