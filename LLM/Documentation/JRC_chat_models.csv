0;1;2;3;4;5;6;7;8;9;10;11;12;13;14;15;16;17;18;19;
('id'; 'llama-3.3-70b-instruct');('created', None);('object', None);('owned_by', None);('access_by_group', None);('model_name', 'llama-3.3-70b-instruct');('chat_mode', True);('streaming', True);('active', True);('has_quota', False);('context_length', 131072);('output_context_length', 13104);('api_only', False);('model_usage', 'chat');('sensitivity_level', ['PA', 'SNC']);('kt_enabled', True);('dev_enabled', False);('knowledge_tools', ['attach_doc', 'thematic_spaces', 'scopus', 'bingnews', 'bing_web_search', 'emm', 'pubsy', 'seta']);('system_prompt', '');"('metadata', {'display_name': 'LLama 3.3 70b instruct', 'model_version': 'llama-3.3', 'best_use': ['The most powerful local model of GPT@JRC. It is the latest Meta model with a performance comparable to GPT-4o. It brings significant improvements over the previous 3.1 version, particularly in programming tasks. Unlimited usage. Best model to be used with PDFs or Word documents, or using knowledge tools.'], 'description': ['Very powerful open-weights model on par with the capabilities of GPT-4o for many types of tasks. It has a context length of 128k tokens, which allows it to work with long documents.', ""More information: <a href='https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct'>meta-llama/Meta-Llama-3.3-70B-Instruct · Hugging Face</a>""], 'license': ""Model license: <a href='https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct/blob/main/LICENSE'>Llama 3.3 license</a>"", 'owned_by': 'meta', 'is_advanced': False, 'is_cloud': False, 'capacities': 3})"
('id'; 'llama-3.1-70b-instruct');('created', None);('object', None);('owned_by', None);('access_by_group', None);('model_name', 'llama-3.1-70b-instruct');('chat_mode', True);('streaming', True);('active', True);('has_quota', False);('context_length', 131072);('output_context_length', 13104);('api_only', False);('model_usage', 'chat');('sensitivity_level', ['PA', 'SNC']);('kt_enabled', True);('dev_enabled', False);('knowledge_tools', ['attach_doc', 'scopus', 'bingnews', 'bing_web_search', 'emm', 'pubsy', 'seta']);('system_prompt', '');"('metadata', {'display_name': 'LLama 3.1 70b instruct', 'model_version': 'llama-3.1', 'best_use': ['Very effective alternative to GPT-4 Turbo. Unlimited usage. It has been superseded by Llama 3.3, which is now the recommended model.'], 'description': ['Very powerful open-weights model on par with the capabilities of GPT-4 for many types of tasks. It has a context length of 128k tokens, which allows it to work with long documents.', ""More information: <a href='https://huggingface.co/meta-llama/Meta-Llama-3.1-70B'>meta-llama/Meta-Llama-3.1-70B-Instruct · Hugging Face</a>""], 'license': ""Model license: <a href='https://huggingface.co/meta-llama/Meta-Llama-3.1-70B/blob/main/LICENSE'>Llama 3.1 license</a>"", 'owned_by': 'meta', 'is_advanced': False, 'is_cloud': False, 'capacities': 3})"
('id'; 'llama-3.1-70b-instruct-fp8');('created', None);('object', None);('owned_by', None);('access_by_group', [{'id': '2ff32750-8c3c-4a13-b3a6-f90ec8c6d22c', 'access_group': 'general'}, {'id': '2e0d3a25-321c-4417-a554-09d505259868', 'access_group': 'jrc-ext-base'}]);('model_name', 'llama-3.1-70b-instruct-fp8');('chat_mode', True);('streaming', True);('active', True);('has_quota', False);('context_length', 131072);('output_context_length', 13104);('api_only', False);('model_usage', 'chat');('sensitivity_level', ['PA', 'SNC']);('kt_enabled', True);('dev_enabled', True);('knowledge_tools', ['attach_doc', 'scopus', 'bingnews', 'bing_web_search', 'emm', 'pubsy', 'seta']);('system_prompt', '');"('metadata', {'display_name': 'LLama 3.1 70b instruct quantized', 'model_version': 'llama-3.1', 'best_use': ['An 8-bit quantized version of the most powerful local model of GPT@JRC in terms of capabilities, with a large context of 128k tokens. Very effective alternative to GPT-4 Turbo. Unlimited usage. Best model to be used with PDFs or Word documents, or using knowledge tools.'], 'description': ['Very powerful open-weights model on par with the capabilities of GPT-4 for many types of tasks. It has a context length of 128k tokens, which allows it to work with long documents.', ""More information: <a href='https://huggingface.co/neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8'>neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8 · Hugging Face</a>""], 'license': ""Model license: <a href='https://huggingface.co/meta-llama/Meta-Llama-3.1-70B/blob/main/LICENSE'>Llama 3.1 license</a>"", 'owned_by': 'neuralmagic', 'is_advanced': False, 'is_cloud': False, 'capacities': 3})"
('id'; 'llama-3.1-8b-instruct');('created', None);('object', None);('owned_by', None);('access_by_group', None);('model_name', 'llama-3.1-8b-instruct');('chat_mode', True);('streaming', True);('active', True);('has_quota', False);('context_length', 131072);('output_context_length', 13104);('api_only', False);('model_usage', 'chat');('sensitivity_level', ['PA', 'SNC']);('kt_enabled', True);('dev_enabled', False);('knowledge_tools', ['attach_doc', 'scopus', 'bingnews', 'bing_web_search', 'emm', 'pubsy', 'seta']);('system_prompt', '');"('metadata', {'display_name': 'LLama 3.1 8b instruct', 'model_version': 'llama-3.1', 'best_use': ['Smaller version of the most powerful local model of GPT@JRC in terms of capabilities, with a large context of 128k tokens. Very effective alternative to LLama3.1 70b, when a faster response is needed. Unlimited usage. Best model to be used with PDFs or Word documents, or using knowledge tools.'], 'description': ['Very powerful open-weights model on par with the capabilities of GPT-4 for many types of tasks. It has a context length of 128k tokens, which allows it to work with long documents.', ""More information: <a href='https://huggingface.co/meta-llama/Meta-Llama-3.1-8B'>meta-llama/Meta-Llama-3.1-8B-Instruct · Hugging Face</a>""], 'license': ""Model license: <a href='https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/LICENSE'>Llama 3.1 license</a>"", 'owned_by': 'meta', 'is_advanced': False, 'is_cloud': False, 'capacities': 2})"
('id'; 'nous-hermes-2-mixtral-8x7b-dpo');('created', None);('object', None);('owned_by', None);('access_by_group', None);('model_name', 'nous-hermes-2-mixtral-8x7b-dpo');('chat_mode', True);('streaming', True);('active', True);('has_quota', False);('context_length', 32768);('output_context_length', 4096);('api_only', False);('model_usage', 'chat');('sensitivity_level', ['PA', 'SNC']);('kt_enabled', True);('dev_enabled', False);('knowledge_tools', ['attach_doc', 'scopus', 'bingnews', 'bing_web_search', 'emm', 'pubsy', 'seta']);"('system_prompt', ""This is a chat system developed at the Joint Research Centre of the European Commission. This system always provides answers to the user's questions to the best of its ability, in a clear and concise way."")";"('metadata', {'display_name': 'Nous Hermes Mixtral', 'model_version': 'v1', 'best_use': ['Best open-source model available in GPT@JRC offering good capabilities and extended context length for working with documents (around 50 pages can be uploaded). Unlimited usage.'], 'description': ['A good opensource alternative to GPT 3.5 Turbo thanks to its good performance and extended context length, up to 32k tokens. The model is a fine-tuned version of Mixtral 8x7b base model, provided by Nous Research.', ""More information: <a href='https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO'>NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO · Hugging Face</a>""], 'license': ""Model license: <a href='https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md'>Apache 2.0</a>"", 'owned_by': 'nous-research', 'is_advanced': False, 'is_cloud': False, 'capacities': 2})"
('id'; 'gpt-4o');('created', None);('object', None);('owned_by', None);('access_by_group', [{'id': '364c8c27-3315-4e5f-87cd-0374c8786573', 'access_group': 'general'}, {'id': '9c56c918-a57f-40c0-8795-ba2865abc0dc', 'access_group': 'gpt-4o-only'}]);('model_name', 'gpt-4o');('chat_mode', True);('streaming', True);('active', True);('has_quota', True);('context_length', 131072);('output_context_length', 4096);('api_only', False);('model_usage', 'chat');('sensitivity_level', ['PA']);('kt_enabled', True);('dev_enabled', False);('knowledge_tools', ['attach_doc', 'thematic_spaces', 'scopus', 'bingnews', 'bing_web_search', 'emm', 'pubsy', 'seta']);('system_prompt', '');"('metadata', {'display_name': 'GPT 4-o', 'model_version': 'gpt-4o', 'best_use': ['Suitable alternative to Llama 3.3 for processing of non-SNC data. Usage limited by daily quota.'], 'description': [""OpenAI's GPT-4o. This model is hosted on Microsoft Azure in the EU with an opt-out clause on prompt analysis by third parties."", ""More information: <a href='https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#gpt-4o-and-gpt-4-turbo'>Azure OpenAI Service models - Azure OpenAI | Microsoft Learn</a>""], 'license': '', 'owned_by': 'openai', 'is_advanced': False, 'is_cloud': True, 'capacities': 3})"
('id'; 'moderation_multilingual');('created', None);('object', None);('owned_by', None);('access_by_group', None);('model_name', 'moderation_multilingual');('chat_mode', False);('streaming', False);('active', True);('has_quota', False);('context_length', 0);('output_context_length', 0);('api_only', True);('model_usage', 'chat');('sensitivity_level', []);('kt_enabled', False);('dev_enabled', False);('knowledge_tools', None);('system_prompt', '');('metadata', {'display_name': 'Moderation', 'model_version': 'v1', 'best_use': ['Moderation model'], 'description': ['Classifier for toxic contents'], 'license': '', 'owned_by': 'unitaryai', 'is_advanced': True, 'is_cloud': False, 'capacities': 0})
('id'; 'qwen-coder-2.5-instruct');('created', None);('object', None);('owned_by', None);('access_by_group', None);('model_name', 'qwen-coder-2.5-instruct');('chat_mode', True);('streaming', True);('active', True);('has_quota', False);('context_length', 32768);('output_context_length', 13104);('api_only', False);('model_usage', 'chat');('sensitivity_level', ['PA', 'SNC']);('kt_enabled', True);('dev_enabled', True);('knowledge_tools', ['attach_doc', 'scopus', 'bingnews', 'bing_web_search', 'emm', 'pubsy', 'seta']);('system_prompt', '');"('metadata', {'display_name': 'Qwen coder 2.5 32b instruct', 'model_version': 'qwen2.5', 'best_use': ['Coder model'], 'description': ['Coder model'], 'license': ""Model license: <a href='https://choosealicense.com/licenses/apache-2.0/'>Apache 2.0</a>"", 'owned_by': 'meta', 'is_advanced': False, 'is_cloud': False, 'capacities': 2})"
('id'; 'qwen-coder-2.5-base');('created', None);('object', None);('owned_by', None);('access_by_group', None);('model_name', 'qwen-coder-2.5-base');('chat_mode', True);('streaming', True);('active', True);('has_quota', False);('context_length', 32768);('output_context_length', 13104);('api_only', True);('model_usage', 'chat');('sensitivity_level', ['PA', 'SNC']);('kt_enabled', True);('dev_enabled', True);('knowledge_tools', ['attach_doc', 'thematic_spaces', 'scopus', 'bingnews', 'bing_web_search', 'emm', 'pubsy', 'seta']);('system_prompt', '');"('metadata', {'display_name': 'Qwen coder 2.5 7b base', 'model_version': 'qwen2.5', 'best_use': ['Coder model'], 'description': ['Coder model'], 'license': ""Model license: <a href='https://choosealicense.com/licenses/apache-2.0/'>Apache 2.0</a>"", 'owned_by': 'meta', 'is_advanced': False, 'is_cloud': False, 'capacities': 2})"